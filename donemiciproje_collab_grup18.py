# -*- coding: utf-8 -*-
"""DonemiciProje_Collab_Grup18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HW_lMWpXfYVysB2uBUVEqYQd_-zX4m2I

Ä°STANBUL SAÄLIK VE TEKNOLOJÄ° ÃœNÄ°VERSÄ°TESÄ°
                        YAZILIM MÃœHENDÄ°SLÄ°ÄÄ°
                  YAPAY ZEKAYA GÄ°RÄ°Å DERSÄ° DÃ–NEMÄ°Ã‡Ä° PROJESÄ°
                    
                    FARE HAREKETLERÄ°NDEN STRES VE HORMONAL
                        DEÄÄ°ÅÄ°MLERÄ°NÄ° EÄÄ°TTÄ°MÄ°Z MODELÄ°MÄ°Z

                  HAZIRLAYANLAR:
                  SELAMÄ° Ã‡ETÄ°N 220609012
                  UÄUR BAKÄ° ARSLAN 220609015
                  YUNUS EMRE SEVÄ°NÃ‡ 220609007

Bu kodumuzda annotations.xml dosyasÄ±nÄ± iÅŸleyerek iÃ§erisindeki iskelet noktalarÄ±nÄ±n koordinatlarÄ±nÄ± her bir zaman dilimi (frame) iÃ§in okuyup bir pandas DataFrame'ine dÃ¶nÃ¼ÅŸtÃ¼rdÃ¼k.
"""

import xml.etree.ElementTree as ET
import pandas as pd

# XML dosyasÄ±nÄ± parse etme
tree = ET.parse('annotations.xml')
root = tree.getroot()
from lxml import etree

parser = etree.XMLParser(recover=True)  # HatalarÄ± gÃ¶rmezden gel
tree = etree.parse("annotations.xml", parser=parser)
# Veriyi depolamak iÃ§in liste
data = []

# Her frame iÃ§in koordinatlarÄ± Ã§ekme
for frame in root.findall('frame'):
    zaman = float(frame.get('zaman'))
    noktalar = {f"x{i}": None for i in range(1, 17)} | {f"y{i}": None for i in range(1, 17)}

    for nokta in frame.findall('nokta'):
        nokta_id = int(nokta.get('id'))
        noktalar[f"x{nokta_id}"] = float(nokta.get('x'))
        noktalar[f"y{nokta_id}"] = float(nokta.get('y'))

    data.append({"zaman": zaman, **noktalar})

# DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼rme
df = pd.DataFrame(data)

import numpy as np  # NumPy kÃ¼tÃ¼phanesini "np" takma adÄ±yla import et
# 1. Merkez nokta (tÃ¼m noktalarÄ±n ortalamasÄ±)
df['x_merkez'] = df.filter(like='x').mean(axis=1)
df['y_merkez'] = df.filter(like='y').mean(axis=1)

# 2. HÄ±z hesaplama (bir Ã¶nceki frame ile)
df['hiz'] = np.sqrt(df['x_merkez'].diff()**2 + df['y_merkez'].diff()**2)

# 3. Donma davranÄ±ÅŸÄ± (hiz < eÅŸik deÄŸer)
df['donma'] = (df['hiz'] < 5).astype(int)

"""Bu kodumuzda, iskelet noktalarÄ±nÄ±n koordinatlarÄ± Ã¼zerinden hareket analizi yaparak, farenin hareket hÄ±zÄ± ve donma (freeze) davranÄ±ÅŸÄ± gibi davranÄ±ÅŸsal ipuÃ§larÄ±nÄ± Ã§Ä±karmaya yÃ¶nelik bir adÄ±mdÄ±r.

ğŸ” Genel MantÄ±ÄŸÄ±:
Kodun amacÄ±, her bir karede (frame'de) farenin ne kadar hareket ettiÄŸini Ã¶lÃ§mek ve bu hareketin Ã§ok az olduÄŸu, yani neredeyse sabit kaldÄ±ÄŸÄ± (donduÄŸu) anlarÄ± tespit etmektir. Bunu ÅŸu ÅŸekilde yapÄ±yor:

TÃ¼m vÃ¼cut noktalarÄ±nÄ±n ortalamasÄ±nÄ± alarak bir merkez pozisyon belirleniyor. Yani "farenin gÃ¶vdesi genel olarak nerede?" sorusuna cevap veriliyor.

ArdÄ±ndan, bu merkez pozisyonlar arasÄ±ndaki fark kullanÄ±larak hareket hÄ±zÄ± hesaplanÄ±yor. Ä°ki kare arasÄ±ndaki pozisyon deÄŸiÅŸimi ne kadar bÃ¼yÃ¼kse, hÄ±z o kadar yÃ¼ksek.

Son olarak, eÄŸer bu hÄ±z belli bir eÅŸikten kÃ¼Ã§Ã¼kse (Ã¶rneÄŸin 5 piksel/frame gibi), bu durum donma (freeze) olarak etiketleniyor. BÃ¶ylece fare uzun sÃ¼re hareket etmediyse bu durum davranÄ±ÅŸsal bir sinyal olarak iÅŸaretlenmiÅŸ oluyor.

ğŸ“Œ Neden Ã¶nemli?
Bu tÃ¼r bilgiler, Ã¶zellikle stres, korku veya anksiyete gibi durumlarÄ±n tahmininde Ã§ok kritiktir Ã§Ã¼nkÃ¼ donma, hayvan davranÄ±ÅŸÄ±nda yaygÄ±n bir stres tepkisidir.
"""

# 1. KÃ¶ÅŸelerde geÃ§irilen sÃ¼re (Ã¶rneÄŸin sol Ã¼st kÃ¶ÅŸe: x<50, y<50)
df['kose_sol_ust'] = ((df['x_merkez'] < 50) & (df['y_merkez'] < 50)).astype(int)

# 2. Alan kullanÄ±mÄ± (kutuyu 4 bÃ¶lgeye ayÄ±rma)
df['bolge'] = pd.cut(df['x_merkez'], bins=[0, 100, 200, 300, 400], labels=[1, 2, 3, 4])

"""Bu kodumuzda, farenin konum verisini kullanarak mekÃ¢nsal davranÄ±ÅŸ analizi yapar. AmaÃ§, hayvanÄ±n test alanÄ± iÃ§inde nerede ne kadar zaman geÃ§irdiÄŸini ve hangi bÃ¶lgeleri daha Ã§ok tercih ettiÄŸini Ã¶lÃ§mektir.

ğŸ§  Genel MantÄ±ÄŸÄ±:
KÃ¶ÅŸelerde geÃ§irilen sÃ¼re:

Farenin sol Ã¼st kÃ¶ÅŸede olup olmadÄ±ÄŸÄ±nÄ± kontrol eder (x < 50 ve y < 50).

EÄŸer bu koÅŸul saÄŸlanÄ±yorsa, o karede hayvanÄ±n sol Ã¼st kÃ¶ÅŸede olduÄŸu kabul edilir ve 1 ile iÅŸaretlenir, aksi halde 0.

Bu bilgi, farenin genellikle gÃ¼venli bÃ¶lgelerde (duvar/kÃ¶ÅŸe) saklanma davranÄ±ÅŸÄ± gÃ¶sterip gÃ¶stermediÄŸini anlamak iÃ§in kullanÄ±lÄ±r. Stresli hayvanlar genellikle kÃ¶ÅŸelere Ã§ekilir.

Alan kullanÄ±mÄ±:

OrtamÄ±n yatay dÃ¼zlemde (x eksenine gÃ¶re) 4 eÅŸit bÃ¶lgeye ayrÄ±lmasÄ± saÄŸlanÄ±r.

pd.cut fonksiyonu ile x_merkez deÄŸerine gÃ¶re her kare bir bÃ¶lgeye atanÄ±r (1. bÃ¶lge: en sol, 4. bÃ¶lge: en saÄŸ).

BÃ¶ylece fare hangi bÃ¶lgelerde daha fazla zaman geÃ§irmiÅŸ kolayca analiz edilebilir.

ğŸ§­ Neden Ã¶nemli?
Farenin hareket ettiÄŸi alanlar davranÄ±ÅŸsal anlam taÅŸÄ±r:

AlanÄ±n sadece belirli bÃ¶lgelerinde vakit geÃ§iren bir fare, kaygÄ±lÄ± ya da merak duygusu dÃ¼ÅŸÃ¼k olabilir.

TÃ¼m alanÄ± aktif kullanan bir fare, daha rahat veya keÅŸfetmeye aÃ§Ä±k olabilir.

Bu tarz mekÃ¢nsal Ã¶zellikler, stres/anksiyete tahminine katkÄ± saÄŸlayan gÃ¼Ã§lÃ¼ davranÄ±ÅŸsal gÃ¶stergelerdir.
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# Set random seed for reproducibility
np.random.seed(42)

# Create synthetic data for 500 mice
n_samples = 500

# Movement features
data = {
    'hareket_hizi': np.random.normal(10, 3, n_samples).clip(5, 20),
    'donma_suresi': np.random.normal(15, 8, n_samples).clip(0, 40),
    'maruz_kalma_suresi': np.random.normal(20, 10, n_samples).clip(5, 50),
    'yemek_yeme_suresi': np.random.normal(100, 30, n_samples).clip(30, 180),
    'sosyal_etkilesim': np.random.normal(50, 20, n_samples).clip(5, 120),
    'kortikosteron': np.random.normal(50, 20, n_samples).clip(20, 100),
    'kalp_atis': np.random.normal(400, 100, n_samples).clip(300, 600),
}

# Create DataFrame
df = pd.DataFrame(data)

# Generate stress levels based on features
conditions = [
    (df['donma_suresi'] < 10) & (df['kortikosteron'] < 40),
    (df['donma_suresi'] >= 20) | (df['kortikosteron'] >= 70),
]
choices = [0, 2]  # 0=Low stress, 2=High stress
df['stres_seviyesi'] = np.select(conditions, choices, default=1)  # 1=Medium stress

# Verify distribution
print("Stress Level Distribution:")
print(df['stres_seviyesi'].value_counts())

# Save to CSV
df.to_csv('Fare.csv', index=False)
print("Dataset saved to 'mouse_stress_data.csv'")

# Prepare for ML
X = df.drop('stres_seviyesi', axis=1)
y = df['stres_seviyesi']

# Split data only if we have samples
if len(X) > 0:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    print(f"\nTrain shape: {X_train.shape}, Test shape: {X_test.shape}")
else:
    print("Error: No samples available for training!")

"""Bu kodumuzda, fare davranÄ±ÅŸ ve fizyolojik verilerini kullanarak stres seviyesi sÄ±nÄ±flandÄ±rmasÄ± iÃ§in yapay bir veri seti Ã¼retir ve bu veriyi makine Ã¶ÄŸrenmesi modeline uygun ÅŸekilde hazÄ±rlar.

ğŸ§  Genel MantÄ±ÄŸÄ±:
ğŸ”§ 1. Sentetik Veri Ãœretimi (SimÃ¼lasyon)
GerÃ§ek deney verisi yerine, istatistiksel olarak benzer (ama rastgele oluÅŸturulmuÅŸ) 500 adet fare gÃ¶zlemi oluÅŸturuluyor.

Her bir fare iÃ§in farklÄ± Ã¶zellikler tanÄ±mlanmÄ±ÅŸ:

hareket_hizi, donma_suresi, maruz_kalma_suresi gibi davranÄ±ÅŸsal;

kortikosteron (stres hormonu) ve kalp_atis gibi fizyolojik veriler.

Bu veriler, normal daÄŸÄ±lÄ±m (Gaussian) temel alÄ±narak rastgele ama belirli aralÄ±klarla Ã¼retiliyor (clip ile sÄ±nÄ±rlandÄ±rÄ±lÄ±yor).

ğŸ” 2. Etiket OluÅŸturma (Stres Seviyesi)
Stres seviyesi, bazÄ± kurallara gÃ¶re belirleniyor:

DÃ¼ÅŸÃ¼k stres (0): Donma sÃ¼resi kÄ±sa ve kortikosteron dÃ¼ÅŸÃ¼k.

YÃ¼ksek stres (2): Donma sÃ¼resi Ã§ok uzun ya da kortikosteron yÃ¼ksek.

DiÄŸer durumlar orta dÃ¼zey stres (1) olarak atanÄ±yor.

np.select() ile bu koÅŸullara gÃ¶re yeni bir stres_seviyesi etiketi ekleniyor.

ğŸ’¾ 3. Veri KaydÄ± ve Modellemeye HazÄ±rlÄ±k
Veri Fare.csv adÄ±yla CSV dosyasÄ±na kaydediliyor.

X (girdi Ã¶zellikleri) ve y (etiket/stres seviyesi) olarak ayrÄ±lÄ±yor.

Veri eÄŸitim ve test seti olarak bÃ¶lÃ¼nÃ¼yor (train_test_split), bÃ¶ylece makine Ã¶ÄŸrenmesi algoritmalarÄ±nda kullanÄ±labilecek duruma getiriliyor.

ğŸ” Bu Neden Ã–nemli?
Bu yapÄ± sayesinde:


FarklÄ± stres dÃ¼zeylerini etkileyen faktÃ¶rleri keÅŸfedebiliriz.

Stres tahminine yÃ¶nelik modelimizi gÃ¼venli bir ÅŸekilde test etmemiz saÄŸlanÄ±r.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split

# Veri setini yÃ¼kle
df = pd.read_csv('Fare.csv')

# 1. Stres Seviyesi DaÄŸÄ±lÄ±mÄ±
plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='stres_seviyesi', palette='viridis')
plt.title('Farelerde Stres Seviyesi DaÄŸÄ±lÄ±mÄ±', fontsize=14)
plt.xlabel('Stres Seviyesi (0=DÃ¼ÅŸÃ¼k, 1=Orta, 2=YÃ¼ksek)')
plt.ylabel('Fare SayÄ±sÄ±')
plt.xticks([0, 1, 2], ['DÃ¼ÅŸÃ¼k', 'Orta', 'YÃ¼ksek'])
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# 2. Stres Seviyesine GÃ¶re Hareket HÄ±zÄ±
plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x='stres_seviyesi', y='hareket_hizi', palette='coolwarm')
plt.title('Stres Seviyesine GÃ¶re Hareket HÄ±zÄ± DaÄŸÄ±lÄ±mÄ±', fontsize=14)
plt.xlabel('Stres Seviyesi')
plt.ylabel('Hareket HÄ±zÄ± (cm/sn)')
plt.xticks([0, 1, 2], ['DÃ¼ÅŸÃ¼k', 'Orta', 'YÃ¼ksek'])
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# 3. Kortikosteron ve Kalp AtÄ±ÅŸÄ± Ä°liÅŸkisi
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='kortikosteron', y='kalp_atis', hue='stres_seviyesi',
               palette='Set2', s=100, alpha=0.8)
plt.title('Stres BiyobelirteÃ§leri Ä°liÅŸkisi', fontsize=14)
plt.xlabel('Kortikosteron Seviyesi (ng/ml)')
plt.ylabel('Kalp AtÄ±ÅŸ HÄ±zÄ± (bpm)')
plt.legend(title='Stres Seviyesi', labels=['DÃ¼ÅŸÃ¼k', 'Orta', 'YÃ¼ksek'])
plt.grid(linestyle='--', alpha=0.5)
plt.show()

# 4. Ã–zelliklerin Korelasyon Matrisi
plt.figure(figsize=(12, 8))
corr_matrix = df.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')
plt.title('Ã–zellikler ArasÄ± Korelasyon Matrisi', fontsize=14)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 5. Donma SÃ¼resi ve Sosyal EtkileÅŸim KarÅŸÄ±laÅŸtÄ±rmasÄ±
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='donma_suresi', y='sosyal_etkilesim',
               hue='stres_seviyesi', palette='tab10', s=80)
plt.title('Donma SÃ¼resi vs Sosyal EtkileÅŸim', fontsize=14)
plt.xlabel('Donma SÃ¼resi (sn)')
plt.ylabel('Sosyal EtkileÅŸim SÃ¼resi (sn)')
plt.legend(title='Stres Seviyesi')
plt.grid(linestyle='--', alpha=0.5)
plt.show()

# 6. Stres Seviyesine GÃ¶re Ã–zelliklerin DaÄŸÄ±lÄ±mÄ± (Violin Plot)
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.violinplot(data=df, x='stres_seviyesi', y='yemek_yeme_suresi', palette='muted')
plt.title('Yemek Yeme SÃ¼resi')
plt.xlabel('Stres Seviyesi')
plt.ylabel('SÃ¼re (sn)')

plt.subplot(1, 2, 2)
sns.violinplot(data=df, x='stres_seviyesi', y='maruz_kalma_suresi', palette='muted')
plt.title('Maruz Kalma SÃ¼resi')
plt.xlabel('Stres Seviyesi')
plt.ylabel('SÃ¼re (sn)')

plt.tight_layout()
plt.show()

"""ğŸ” Genel AmaÃ§: Farelerde Stresin GÃ¶rselleÅŸtirilmesi

Bu kodumuzda, elimizdeki veriye sadece sayÄ±lar olarak deÄŸil, gÃ¶rsel olarak bakmamÄ±zÄ± saÄŸlÄ±yor. BÃ¶ylece, â€œhangi davranÄ±ÅŸlar veya biyolojik Ã¶lÃ§Ã¼mler stresle baÄŸlantÄ±lÄ±?â€ sorusuna grafiklerle cevap bulmaya Ã§alÄ±ÅŸÄ±yoruz.

Bu kodun arkasÄ±ndaki temel mantÄ±k ÅŸu:

âœ… 1. Verinin Dengesi ve DaÄŸÄ±lÄ±mÄ±nÄ± Anlamak
Ä°lk grafik (countplot), hangi stres seviyesinden kaÃ§ fare var onu gÃ¶steriyor.

AmaÃ§: Veride dengesizlik var mÄ±? Ã–rneÄŸin sadece yÃ¼ksek stresli fareler fazlaysa, model buna kayabilir.

âœ… 2. DavranÄ±ÅŸlarÄ±n Stresle NasÄ±l DeÄŸiÅŸtiÄŸini GÃ¶rmek
Kutu ve violin grafikler (boxplot, violinplot), belirli bir davranÄ±ÅŸÄ±n (Ã¶rneÄŸin hareket hÄ±zÄ±, yemek sÃ¼resi) stres seviyesi arttÄ±kÃ§a nasÄ±l deÄŸiÅŸtiÄŸini gÃ¶steriyor.

AmaÃ§: "Stres arttÄ±kÃ§a fare daha az mÄ± hareket ediyor, daha az mÄ± yiyor?" gibi sorulara yanÄ±t bulmak.

âœ… 3. Biyolojik Belirtiler ArasÄ±ndaki Ä°liÅŸkiyi GÃ¶zlemlemek
Kortikosteron ve kalp atÄ±ÅŸ hÄ±zÄ± gibi biyobelirteÃ§ler iÃ§in scatter plot kullanÄ±lÄ±yor.

AmaÃ§: â€œBu iki deÄŸer birlikte artÄ±yor mu? Hangi stres grubunda yoÄŸunluk var?â€ gibi iliÅŸkileri gÃ¶rmek.

âœ… 4. TÃ¼m Ã–zellikler Birbirleriyle NasÄ±l Ä°liÅŸkili?
IsÄ± haritasÄ± (korelasyon matrisi), tÃ¼m Ã¶zelliklerin birbiriyle olan baÄŸlantÄ±sÄ±nÄ± gÃ¶steriyor.

AmaÃ§: Belki iki farklÄ± deÄŸiÅŸken aslÄ±nda aynÄ± bilgiyi taÅŸÄ±yor (yÃ¼ksek korelasyon). Bu durumda modelde ikisine birden gerek olmayabilir.

âœ… 5. Stres Seviyelerine GÃ¶re Gruplar AyÄ±rt Edilebiliyor mu?
Grafikler yardÄ±mÄ±yla stres seviyesi arttÄ±ÄŸÄ±nda gÃ¶zle gÃ¶rÃ¼lÃ¼r deÄŸiÅŸimler var mÄ±? sorusunu inceliyoruz.

AmaÃ§: EÄŸer gÃ¶rsel olarak stres seviyeleri ayÄ±rt edilebiliyorsa, model de bunu Ã¶ÄŸrenebilir.

ğŸ¯ Ã–zetle:
Bu grafiklerle:

Veriyi tanÄ±yoruz,

Hangi deÄŸiÅŸkenlerin Ã¶nemli olabileceÄŸini anlÄ±yoruz,

Modelleme Ã¶ncesi Ã¶n keÅŸif analizi (exploratory data analysis â€“ EDA) yapÄ±yoruz.

Yani bunlar sadece sÃ¼s deÄŸil â€” veri biliminin ilk adÄ±mÄ±nda yapÄ±lmasÄ± gereken zorunlu bir analizdir.
"""

import xml.etree.ElementTree as ET
import pandas as pd
import numpy as np

# XML'den veri Ã§ekme
tree = ET.parse('annotations.xml')
root = tree.getroot()

data = []
for frame in root.findall('frame'):
    frame_id = frame.get('id')
    if frame_id is None:
        continue  # ID yoksa bu frame'i atla
    frame_data = {'frame_id': int(frame_id)}

    for point in frame.findall('point'):
        pid = point.get('id')
        x = point.get('x')
        y = point.get('y')
        if pid is None or x is None or y is None:
            continue
        pid = int(pid)
        frame_data[f'x{pid}'] = float(x)
        frame_data[f'y{pid}'] = float(y)

    data.append(frame_data)

# DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼rme
df_xml = pd.DataFrame(data)

# EÄŸer 'frame_id' index olmuÅŸsa, onu sÃ¼tun olarak al
if 'frame_id' not in df_xml.columns and df_xml.index.name == 'frame_id':
    df_xml.reset_index(inplace=True)

# Ã–zellik mÃ¼hendisliÄŸi
df_xml['center_x'] = df_xml.filter(like='x').mean(axis=1)
df_xml['center_y'] = df_xml.filter(like='y').mean(axis=1)
df_xml['frame_id']=df_xml.filter(like='frame_id').mean(axis=1)

# Hareket hÄ±zÄ± hesaplama (30 FPS)
df_xml['speed'] = np.sqrt(
    df_xml['center_x'].diff()**2 +
    df_xml['center_y'].diff()**2
) /(df_xml['frame_id'].diff() / 30)  # 30 FPS varsayÄ±mÄ±

# Donma davranÄ±ÅŸÄ±
df_xml['freezing'] = (df_xml['speed'] < 5).astype(int)

# Ã–zet Ã¶zellikler
xml_features = df_xml.groupby('frame_id').agg({
    'speed': ['mean', 'std'],
    'freezing': 'sum'
})
xml_features.columns = ['speed_mean', 'speed_std', 'freezing_count']


# CSV verisini yÃ¼kle
df_csv = pd.read_csv('Fare.csv')

df_combined = pd.merge(
    left=df_csv,
    right=xml_features,
    left_on='hareket_hizi',
    right_index=True,
    how='inner'
)

# Eksik verileri temizleme
df_combined.dropna(inplace=True)
# Verileri birleÅŸtirme (frame_id Ã¼zerinden)

"""ğŸ” Bu kodun amacÄ± ne?
Bizim elimizde iki farklÄ± verimiz vardÄ±:

Fare davranÄ±ÅŸlarÄ± ve biyobelirteÃ§leri (Ã¶rneÄŸin kortikosteron, kalp atÄ±ÅŸÄ± vs.) â†’ CSV dosyasÄ±ndan geliyor.

Fare iskelet hareketleri (yani farenin videosundan Ã§Ä±karÄ±lmÄ±ÅŸ iskelet noktalarÄ±) â†’ XML dosyasÄ±ndan geliyor.

Bu iki veri, stres gibi iÃ§sel durumlarÄ± daha iyi anlamak iÃ§in birleÅŸtiriliyor.

ğŸ’¡ Peki bu nasÄ±l yapÄ±lÄ±yor? Genel MantÄ±k:
1. Fare hareketleri analiz ediliyor.
Fare her karede (frame) farklÄ± pozisyonlarda. Her karenin iÃ§inde iskelet noktalarÄ±nÄ±n x ve y koordinatlarÄ± var.

ğŸ‘‰ Bu noktalardan farenin "vÃ¼cudunun merkezi" hesaplanÄ±yor.

2. Hareket hÄ±zÄ± bulunuyor.
Ä°ki kare arasÄ±ndaki pozisyon farkÄ± hesaplanÄ±yor. Bu, farenin ne kadar hÄ±zlÄ± hareket ettiÄŸini gÃ¶steriyor.

Ã‡Ã¼nkÃ¼ stresli hayvanlar ya donup kalÄ±r ya da Ã§ok hareket eder.

3. Donma davranÄ±ÅŸÄ± tespit ediliyor.
EÄŸer hareket hÄ±zÄ± Ã§ok dÃ¼ÅŸÃ¼kse (Ã¶rneÄŸin 5 cm/sn'den azsa), bu kare "donmuÅŸ" kabul ediliyor.

Yani fare hiÃ§ kÄ±pÄ±rdamamÄ±ÅŸ gibi.

Bu davranÄ±ÅŸ sayÄ±lÄ±yor. KaÃ§ karede donma olmuÅŸ? Bu bilgi tutuluyor.

4. Ã–zet Ã§Ä±karÄ±lÄ±yor.
TÃ¼m kareler iÃ§in ortalama hÄ±z, hÄ±zÄ±n deÄŸiÅŸkenliÄŸi (kararsÄ±zlÄ±k) ve toplam donma sÃ¼resi gibi Ã¶zet bilgiler Ã¼retiliyor.

Bu bilgiler artÄ±k tek tek kareler deÄŸil, bir bireyin genel stres hareket Ã¶rÃ¼ntÃ¼sÃ¼nÃ¼ yansÄ±tÄ±yor.

5. Bu veriler diÄŸer dosyayla birleÅŸtiriliyor.
Fare hakkÄ±nda baÅŸka bilgiler de var: Kortikosteron, kalp atÄ±ÅŸÄ±, sosyal etkileÅŸim sÃ¼resi, vb.

Åimdi elimizde hem hareket verisi, hem de biyolojik/davranÄ±ÅŸsal veri oldu. BunlarÄ± birleÅŸtiriyoruz.

BÃ¶ylece elimizde ÅŸu tÃ¼r bir veri seti oluÅŸuyor:


fare	kortikosteron	kalp_atis	hareket_hizi	donma_suresi	...
Bu birleÅŸik veriyle stres tahmini, sÄ±nÄ±flandÄ±rma ya da istatistiksel analiz Ã§ok daha gÃ¼Ã§lÃ¼ hale geliyor.

ğŸ” Genel BakÄ±ÅŸ:
Bu iÅŸin mantÄ±ÄŸÄ± ÅŸu:

Ham hareket verisini yorumlanabilir, Ã¶zet davranÄ±ÅŸlara dÃ¶nÃ¼ÅŸtÃ¼r ve bunu biyolojik verilerle birleÅŸtir. BÃ¶ylece stres gibi soyut kavramlarÄ± Ã¶lÃ§ebilen bir sistem kur.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

np.random.seed(42)

# Hareket verisi
num_frames = 200
df_xml = pd.DataFrame({
    'frame': range(num_frames),
    'speed': np.random.normal(10, 3, num_frames).cumsum()/10,
    'x_position': np.random.normal(0, 1, num_frames).cumsum(),
    'y_position': np.random.normal(0, 1, num_frames).cumsum()
})

# Stres seviyelerine gÃ¶re veri
stres_levels = ['DÃ¼ÅŸÃ¼k', 'Orta', 'YÃ¼ksek']
num_groups = len(stres_levels)
num_mice_per_group = 15

df_combined = pd.DataFrame({
    'fare_id': [f'Fare_{i}' for i in range(1, num_groups*num_mice_per_group +1)],
    'stres_seviyesi': np.repeat(stres_levels, num_mice_per_group),
    'speed_mean': np.concatenate([
        np.random.normal(8, 1, num_mice_per_group),
        np.random.normal(12, 1.5, num_mice_per_group),
        np.random.normal(6, 2, num_mice_per_group)
    ]),
    'speed_std': np.concatenate([
        np.random.normal(2, 0.5, num_mice_per_group),
        np.random.normal(4, 0.7, num_mice_per_group),
        np.random.normal(5, 1, num_mice_per_group)
    ]),
    'kortikosteron': np.concatenate([
        np.random.normal(150, 20, num_mice_per_group),
        np.random.normal(250, 30, num_mice_per_group),
        np.random.normal(350, 40, num_mice_per_group)
    ]),
    'kalp_atis': np.concatenate([
        np.random.normal(500, 50, num_mice_per_group),
        np.random.normal(600, 60, num_mice_per_group),
        np.random.normal(700, 70, num_mice_per_group)
    ]),
    'freezing_count': np.concatenate([
        np.random.poisson(5, num_mice_per_group),
        np.random.poisson(15, num_mice_per_group),
        np.random.poisson(25, num_mice_per_group)
    ])
})

# Grafikler
plt.figure(figsize=(15, 10))

# 1. Hareket Ã–rÃ¼ntÃ¼leri
plt.subplot(2, 2, 1)
sns.lineplot(data=df_xml, x="frame", y='speed')
plt.title('Fare Hareket HÄ±zÄ±')
plt.xlabel('Frame')
plt.ylabel('HÄ±z (piksel/frame)')

# 2. Stres-Seviye Ä°liÅŸkisi
plt.subplot(2, 2, 2)
sns.boxplot(data=df_combined, x='stres_seviyesi', y='speed_mean')
plt.title('Stres Seviyesine GÃ¶re Ortalama HÄ±z')
plt.xlabel('Stres Seviyesi')
plt.ylabel('Ortalama HÄ±z')

# 3. BiyobelirteÃ§ Korelasyonu
plt.subplot(2, 2, 3)
sns.heatmap(df_combined[['kortikosteron', 'kalp_atis', 'freezing_count']].corr(),
            annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('BiyobelirteÃ§ KorelasyonlarÄ±')

# 4. Hareket DaÄŸÄ±lÄ±mÄ±
plt.subplot(2, 2, 4)
sns.scatterplot(data=df_combined, x='speed_mean', y='speed_std',
               hue='stres_seviyesi', palette='viridis', s=100)
plt.title('Hareket DeÄŸiÅŸkenliÄŸi')
plt.xlabel('Ortalama HÄ±z')
plt.ylabel('HÄ±z Standart SapmasÄ±')
plt.legend(title='Stres Seviyesi')

plt.tight_layout()
plt.show()

"""Bu kodun mantÄ±ÄŸÄ±nÄ± adÄ±m adÄ±m anlatacak olursak, burada iki ana veri seti ile Ã§alÄ±ÅŸÄ±yoruz:

Fare hareket verisi (XML veri): Bu, farenin her karedeki hareketini (x ve y koordinatlarÄ±) ve hÄ±zÄ±nÄ± temsil ediyor.

Fare biyobelirteÃ§ verisi (CSV veri): Bu, farelerin stres seviyeleri ve biyolojik verileri (kortikosteron, kalp atÄ±ÅŸÄ±, donma sÃ¼resi gibi) iÃ§eriyor.

Kod, bu iki veri setinden Ã§eÅŸitli grafikler oluÅŸturuyor ve farelerin stres seviyelerine gÃ¶re hareket verilerini ve biyobelirteÃ§leri analiz ediyor. Åimdi, her adÄ±mÄ± ve yapÄ±lan iÅŸlemi aÃ§Ä±klayalÄ±m.

1. Fare Hareket Verisi (XML)
Ä°lk olarak, df_xml veri Ã§erÃ§evesi oluÅŸturuluyor. Bu Ã§erÃ§eve, farelerin her bir karedeki hareketlerini temsil ediyor. Her kare iÃ§in hÄ±z, x ve y koordinatlarÄ± rastgele Ã¼retiliyor.

HÄ±z (speed): Hareketin toplam hÄ±zÄ±nÄ±, yani kareler arasÄ±ndaki mesafeyi temsil ediyor.

x ve y pozisyonlarÄ±: Bu deÄŸerler farelerin her bir karedeki x ve y koordinatlarÄ±.

df_xml ÅŸu ÅŸekilde oluÅŸturuluyor:

HÄ±z, rastgele bir normal daÄŸÄ±lÄ±mdan tÃ¼retiliyor ve her bir kare iÃ§in birikimli toplamÄ± alÄ±narak "kÃ¼mÃ¼latif hÄ±z" oluÅŸturuluyor.

X ve Y pozisyonlarÄ±, rastgele deÄŸerlerle oluÅŸturuluyor ve bu deÄŸerler birbirleriyle iliÅŸkilendiriliyor.

Grafik 1: Hareket HÄ±zÄ± Ã–rÃ¼ntÃ¼sÃ¼
sns.lineplot: Farelerin hareket hÄ±zÄ±nÄ± her bir kare iÃ§in Ã§iziyor.

X ekseninde frame (Ã§erÃ§eve numarasÄ±), Y ekseninde hÄ±z (piksel/frame) yer alÄ±yor.

2. Farelerin Stres Seviyesine GÃ¶re HÄ±z DaÄŸÄ±lÄ±mÄ±
Burada, farelerin stres seviyelerine gÃ¶re ortalama hÄ±z (speed_mean) gÃ¶steriliyor.

Stres Seviyesi: DÃ¼ÅŸÃ¼k, Orta, YÃ¼ksek olarak 3 grup var.

HÄ±z verisi her grupta farklÄ± ÅŸekilde rastgele tÃ¼retilmiÅŸ:

DÃ¼ÅŸÃ¼k stresli fareler iÃ§in daha dÃ¼ÅŸÃ¼k hÄ±z (8Â±1),

Orta stresli fareler iÃ§in daha yÃ¼ksek hÄ±z (12Â±1.5),

YÃ¼ksek stresli fareler iÃ§in ise daha dÃ¼ÅŸÃ¼k hÄ±z (6Â±2).

Grafik 2: Stres Seviyesine GÃ¶re Ortalama HÄ±z
sns.boxplot: Stres seviyelerine gÃ¶re farelerin ortalama hÄ±zlarÄ±nÄ± gÃ¶steren bir kutu grafik.

X ekseninde stres seviyesi, Y ekseninde ortalama hÄ±z var.

3. BiyobelirteÃ§ Korelasyonu
Burada, kortikosteron, kalp atÄ±ÅŸÄ± ve donma sayÄ±sÄ± arasÄ±ndaki korelasyon inceleniyor. Korelasyon, bu biyolojik verilerin birbirleriyle olan iliÅŸkisini gÃ¶sterir.

Korelasyon: -1 ile 1 arasÄ±nda deÄŸiÅŸen bir deÄŸer olup, 1 pozitif, -1 negatif ve 0 iliÅŸkisiz olduÄŸunu gÃ¶sterir.

Grafik 3: BiyobelirteÃ§ KorelasyonlarÄ±
sns.heatmap: Korelasyon matrisini gÃ¶rselleÅŸtiriyor. BiyobelirteÃ§lerin birbirleriyle nasÄ±l iliÅŸkili olduklarÄ± hakkÄ±nda bilgi veriyor.

4. Hareket DaÄŸÄ±lÄ±mÄ±
Burada, farelerin ortalama hÄ±z ve hÄ±zÄ±n standart sapmasÄ± arasÄ±ndaki iliÅŸkiyi gÃ¶steren bir daÄŸÄ±lÄ±m grafiÄŸi var.

HÄ±z Standart SapmasÄ±: Hareketin ne kadar deÄŸiÅŸken olduÄŸunu gÃ¶sterir. YÃ¼ksek bir standart sapma, farelerin hÄ±zÄ±nÄ±n Ã§ok deÄŸiÅŸken olduÄŸunu gÃ¶sterir (Ã¶rneÄŸin, Ã§ok hÄ±zlÄ± ve Ã§ok yavaÅŸ hareket etmeleri).

Ortalama HÄ±z: Yani, farelerin genel hareket hÄ±zÄ±.

Grafik 4: Hareket DeÄŸiÅŸkenliÄŸi
sns.scatterplot: Farelerin ortalama hÄ±z ve hÄ±zÄ±n standart sapmasÄ± arasÄ±ndaki iliÅŸkiyi noktalarla Ã§iziyor.

HÄ±zÄ±n standart sapmasÄ± yatay eksende, ortalama hÄ±z dikey eksende yer alÄ±yor.

Renkler, farelerin stres seviyesi ile gruplandÄ±rÄ±lÄ±yor.

Genel Analiz ve SonuÃ§
Bu kodda, farelerin stres seviyelerine gÃ¶re hÄ±zlarÄ± ve biyobelirteÃ§lerin korelasyonlarÄ± detaylÄ± ÅŸekilde inceleniyor:

Hareket hÄ±zlarÄ± zaman iÃ§inde nasÄ±l deÄŸiÅŸiyor.

Stres seviyeleri ile farelerin ortalama hÄ±zlarÄ± arasÄ±ndaki farklar.

BiyobelirteÃ§lerin (kortikosteron, kalp atÄ±ÅŸÄ±, donma sÃ¼resi) birbirleriyle nasÄ±l iliÅŸkilendiÄŸi.

Hareketin deÄŸiÅŸkenliÄŸi (farklÄ± hÄ±zlar arasÄ±nda ne kadar dalgalanma var).

Bu veriler, farelerin stres seviyelerini daha iyi anlamak ve potansiyel olarak stresin hareket Ã¼zerindeki etkisini incelemek iÃ§in kullandÄ±ÄŸÄ±mÄ±z verilerimizdir.
"""

import pandas as pd
import xml.etree.ElementTree as ET

def parse_xml(xml_file):
    tree = ET.parse(xml_file)
    root = tree.getroot()

    data = []

    for track in root.findall('.//track'):
        if track.attrib.get('label') != 'rat_skeleton':
            continue

        for skeleton in track.findall('skeleton'):
            frame_id = skeleton.attrib.get('frame')
            if frame_id is None:
                continue

            for point in skeleton.findall('points'):
                label = point.get('label')
                points = point.get('points')

                if points is None or label is None:
                    continue

                x, y = map(float, points.split(','))
                data.append({
                    'frame_id': int(frame_id),
                    'label': int(label),
                    'x': x,
                    'y': y
                })

    return pd.DataFrame(data)

def main(xml_path, csv_path):
    xml_df = parse_xml(xml_path)
    csv_df = pd.read_csv(csv_path)

    # SÄ±raya gÃ¶re (index bazlÄ±) birleÅŸtirme
    xml_df = xml_df.reset_index(drop=True)
    csv_df = csv_df.reset_index(drop=True)

    min_len = min(len(xml_df), len(csv_df))  # FarklÄ± uzunluk varsa kÄ±sa olan kadar al

    merged = pd.concat([xml_df.iloc[:min_len], csv_df.iloc[:min_len]], axis=1)

    print("\nâœ… BirleÅŸtirilmiÅŸ veri (ilk 5 satÄ±r):")
    print(merged.head(200))

    # CSV olarak kaydetmek istersen:
    merged.to_csv("birlesik_veri.csv", index=False)

if __name__ == "__main__":
    main("annotations.xml", "Fare.csv")

"""Bu kod parametremizde, XML dosyasÄ±ndaki fare hareket verileri ile CSV dosyasÄ±ndaki veriyi birleÅŸtiren bir iÅŸ akÄ±ÅŸÄ±nÄ± tanÄ±mlar. Kod, XML dosyasÄ±ndaki fare iskelet noktalarÄ±nÄ± okur ve bunlarÄ± pandas veri Ã§erÃ§evesine aktarÄ±r. Daha sonra CSV dosyasÄ±ndaki verilerle bu Ã§erÃ§eveyi sÄ±rayla birleÅŸtirir.

DetaylÄ± aÃ§Ä±klamayÄ± adÄ±m adÄ±m yapalÄ±m:

AdÄ±m 1: XML DosyasÄ±nÄ±n ParÃ§alanmasÄ±
parse_xml Fonksiyonu
ET.parse(xml_file): XML dosyasÄ±nÄ± analiz etmek iÃ§in kullanÄ±lan ElementTree modÃ¼lÃ¼nÃ¼n bir fonksiyonu. Bu fonksiyon XML dosyasÄ±nÄ± aÃ§ar ve bir aÄŸaÃ§ yapÄ±sÄ± olarak iÅŸler.

root = tree.getroot(): XML dosyasÄ±nÄ±n kÃ¶k elementini alÄ±r (genellikle kÃ¶k etiketin adÄ±).

track.findall('.//track'): TÃ¼m <track> etiketlerini arar (bu, her bir fare iÃ§in veri iÃ§eren elementtir).

if track.attrib.get('label') != 'rat_skeleton': Etiketin rat_skeleton olup olmadÄ±ÄŸÄ±nÄ± kontrol eder. Bu, sadece fare iskeletini iÃ§eren verileri almak iÃ§in yapÄ±lÄ±r.

skeleton.findall('skeleton'): <skeleton> etiketlerini arar ve her birinin iÃ§indeki verileri iÅŸler.

frame_id = skeleton.attrib.get('frame'): Ã‡erÃ§eve ID'sini alÄ±r (hareketin zamanÄ±nÄ± belirten bir bilgi).

points.split(','): NoktalarÄ± virgÃ¼lle ayÄ±rarak x ve y koordinatlarÄ±nÄ± alÄ±r.

Veri YapÄ±sÄ±
Her bir iskeletin noktalarÄ± (x, y koordinatlarÄ±) ve etiketler (Ã¶rneÄŸin, hangi iskelet noktasÄ± olduÄŸu) Ã§Ä±karÄ±lÄ±r. Bu veriler bir pandas DataFrame'ine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r ve her bir satÄ±rda:

frame_id: Ã‡erÃ§eve kimliÄŸi (int)

label: Etiket (int)

x: x koordinatÄ± (float)

y: y koordinatÄ± (float)

AdÄ±m 2: CSV DosyasÄ±nÄ±n OkunmasÄ±
csv_df = pd.read_csv(csv_path): CSV dosyasÄ±nÄ± okur ve pandas veri Ã§erÃ§evesine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.

AdÄ±m 3: Verilerin BirleÅŸtirilmesi
xml_df = xml_df.reset_index(drop=True) ve csv_df = csv_df.reset_index(drop=True): Hem XML hem de CSV veri Ã§erÃ§evelerinin indekslerini sÄ±fÄ±rlar. Bu, her iki veri setinin sÄ±rasÄ±yla birleÅŸtirilmesine yardÄ±mcÄ± olur.

min_len = min(len(xml_df), len(csv_df)): XML ve CSV veri Ã§erÃ§evelerinin uzunluklarÄ±nÄ±n en kÃ¼Ã§Ã¼ÄŸÃ¼nÃ¼ bulur. Bu, iki veri Ã§erÃ§evesinin birbirine uygun ÅŸekilde birleÅŸtirilmesi iÃ§in gereklidir (uzunluklar farklÄ±ysa, kÄ±sa olan kadar alÄ±nÄ±r).

merged = pd.concat([xml_df.iloc[:min_len], csv_df.iloc[:min_len]], axis=1): XML ve CSV veri Ã§erÃ§evelerini yan yana (sÃ¼tun bazlÄ±) birleÅŸtirir.

AdÄ±m 4: SonuÃ§larÄ±n Kaydedilmesi ve GÃ¶rÃ¼ntÃ¼lenmesi
merged.head(200): BirleÅŸtirilen verilerin ilk 200 satÄ±rÄ±nÄ± yazdÄ±rÄ±r. Bu, verinin doÄŸru ÅŸekilde birleÅŸtirilip birleÅŸtirilmediÄŸini kontrol etmek iÃ§in kullanÄ±lÄ±r.

merged.to_csv("birlesik_veri.csv", index=False): BirleÅŸtirilen veriyi bir CSV dosyasÄ±na kaydeder.

Kodun KullanÄ±mÄ±:
main("annotations.xml", "Fare.csv"): XML ve CSV dosyalarÄ±nÄ±n yollarÄ±nÄ± belirtir ve programÄ± Ã§alÄ±ÅŸtÄ±rÄ±r.

"annotations.xml": Farelerin hareket verilerini iÃ§eren XML dosyasÄ±nÄ±n yolu.

"Fare.csv": CSV dosyasÄ±ndaki ek verilerin bulunduÄŸu dosya.

SonuÃ§lar:
Kod, XML dosyasÄ±ndaki fare iskelet verisini ve CSV dosyasÄ±ndaki fare bilgilerini birleÅŸtirerek yeni bir veri seti oluÅŸturduk.

BirleÅŸtirilmiÅŸ veri, farelerin hareket verilerini ve biyolojik parametreleri iÃ§erir ve CSV formatÄ±nda kaydedilebilir.
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# 1. VERÄ°YÄ° YÃœKLEME VE Ã–N Ä°ÅLEME
data = pd.read_csv("birlesik_veri.csv")

# Eksik verileri kontrol et
print("Eksik veri kontrolÃ¼:\n", data.isnull().sum())

# Eksik verileri doldurma/silme
data = data.dropna()  # Veya data.fillna(method='ffill')

# 2. Ã–ZELLÄ°K MÃœHENDÄ°SLÄ°ÄÄ°
# VÃ¼cut noktalarÄ±na gÃ¶re Ã¶zellikler oluÅŸturma
features = data.groupby('frame_id').agg({
    'x': ['mean', 'std', 'min', 'max'],  # X koordinat istatistikleri
    'y': ['mean', 'std', 'min', 'max'],  # Y koordinat istatistikleri
    'label': 'nunique'                   # EÅŸsiz nokta sayÄ±sÄ±
}).reset_index()

# SÃ¼tun isimlerini dÃ¼zenle
features.columns = ['frame_id', 'x_mean', 'x_std', 'x_min', 'x_max',
                   'y_mean', 'y_std', 'y_min', 'y_max', 'unique_points']

# Stres seviyelerini birleÅŸtirme
stress_levels = data[['frame_id', 'stres_seviyesi']].drop_duplicates()
features = pd.merge(features, stress_levels, on='frame_id')

# 3. GÃ–RSELLEÅTÄ°RME
plt.figure(figsize=(15, 10))

# Stres seviyesi daÄŸÄ±lÄ±mÄ±
plt.subplot(2, 2, 1)
sns.countplot(data=features, x='stres_seviyesi', palette='viridis')
plt.title('Stres Seviyesi DaÄŸÄ±lÄ±mÄ±')

# Hareket Ã¶zellikleri
plt.subplot(2, 2, 2)
sns.scatterplot(data=features, x='x_mean', y='y_mean',
               hue='stres_seviyesi', palette='coolwarm')
plt.title('Ortalama Pozisyon ve Stres Ä°liÅŸkisi')

# Korelasyon matrisi
plt.subplot(2, 2, 3)
sns.heatmap(features.corr(), annot=True, cmap='coolwarm')
plt.title('Ã–zellik KorelasyonlarÄ±')

plt.tight_layout()
plt.show()

# 4. MODEL OLUÅTURMA
X = features.drop(['frame_id', 'stres_seviyesi'], axis=1)
y = features['stres_seviyesi']

# Veriyi bÃ¶lme
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

# Model eÄŸitimi
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 5. DEÄERLENDÄ°RME
y_pred = model.predict(X_test)

print("\nğŸ“Š SÄ±nÄ±flandÄ±rma Raporu:")
print(classification_report(y_test, y_pred))

# KarÄ±ÅŸÄ±klÄ±k matrisi
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['DÃ¼ÅŸÃ¼k', 'Orta', 'YÃ¼ksek'],
            yticklabels=['DÃ¼ÅŸÃ¼k', 'Orta', 'YÃ¼ksek'])
plt.title('KarÄ±ÅŸÄ±klÄ±k Matrisi')
plt.xlabel('Tahmin')
plt.ylabel('GerÃ§ek')
plt.show()

# 6. Ã–ZELLÄ°K Ã–NEMLERÄ°
plt.figure(figsize=(10, 6))
importances = pd.Series(model.feature_importances_, index=X.columns)
importances.sort_values().plot.barh(color='teal')
plt.title('Ã–zellik Ã–nem Dereceleri')
plt.show()

# 7. MODELÄ° KAYDETME
import joblib
joblib.dump(model, 'stres_tahmin_modeli.pkl')
print("\nâœ… Model baÅŸarÄ±yla kaydedildi: 'stres_tahmin_modeli.pkl'")

"""Bu kodumuz modelimiz inÅŸa ettiÄŸimiz nihayi kodumuzdur, fare iskelet hareketlerine dayalÄ± olarak stres seviyesini tahmin etmeye yÃ¶nelik bir makine Ã¶ÄŸrenmesi modelini oluÅŸturduÄŸumuz modelimizi adÄ±m adÄ±m aÃ§Ä±klamalarÄ± yapalÄ±m:

AdÄ±m 1: Veri YÃ¼kleme ve Ã–n Ä°ÅŸleme
Veri YÃ¼kleme
data = pd.read_csv("birlesik_veri.csv"): BirleÅŸtirilmiÅŸ veriyi CSV dosyasÄ±ndan pandas veri Ã§erÃ§evesine (DataFrame) okur.

Eksik Verileri Kontrol Etme
data.isnull().sum(): Eksik verilerin sayÄ±sÄ±nÄ± kontrol eder. EÄŸer verilerde eksik deÄŸerler varsa, bunlar uygun bir ÅŸekilde iÅŸlenmelidir.

Eksik Verileri Doldurma/Silme
data = data.dropna(): Eksik verileri iÃ§eriyorsa, bu satÄ±rlarÄ± veri Ã§erÃ§evesinden Ã§Ä±karÄ±r. Alternatif olarak, eksik veriler fillna() fonksiyonu ile doldurulabilir.

AdÄ±m 2: Ã–zellik MÃ¼hendisliÄŸi
VÃ¼cut NoktalarÄ±na GÃ¶re Ã–zellikler
features = data.groupby('frame_id').agg(): Bu adÄ±mda her bir Ã§erÃ§eve iÃ§in, fare hareket noktalarÄ±na dair bazÄ± temel istatistikler hesaplanÄ±r:

mean: Ortalama konum

std: Standart sapma

min: En dÃ¼ÅŸÃ¼k konum

max: En yÃ¼ksek konum

nunique: EÅŸsiz nokta sayÄ±sÄ±

Bu istatistikler, fare hareketlerinin zaman iÃ§indeki deÄŸiÅŸkenliklerini daha iyi anlamamÄ±za yardÄ±mcÄ± olur.

SÃ¼tun Ä°simlerinin DÃ¼zenlenmesi
features.columns: Her bir Ã¶zelliÄŸin adÄ±, anlaÅŸÄ±labilir olmasÄ± iÃ§in yeniden dÃ¼zenlenir.

Stres Seviyelerinin BirleÅŸtirilmesi
stress_levels = data[['frame_id', 'stres_seviyesi']].drop_duplicates(): Stres seviyelerini ve her Ã§erÃ§eveye ait IDâ€™leri alÄ±r.

features = pd.merge(features, stress_levels, on='frame_id'): Ã–zelliklerin bulunduÄŸu DataFrame ile stres seviyesi bilgileri birleÅŸtirilir.

AdÄ±m 3: GÃ¶rselleÅŸtirme
Stres Seviyesi DaÄŸÄ±lÄ±mÄ±
sns.countplot(): Stres seviyelerinin daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶rselleÅŸtirir (kaÃ§ Ã§erÃ§eve hangi stres seviyesinde).

Ortalama Pozisyon ve Stres Ä°liÅŸkisi
sns.scatterplot(): Hareketin ortalama konumuna (x_mean, y_mean) gÃ¶re stres seviyesini gÃ¶rselleÅŸtirir.

Korelasyon Matrisi
sns.heatmap(): Ã–zellikler arasÄ±ndaki korelasyonu gÃ¶rselleÅŸtirir.

AdÄ±m 4: Model OluÅŸturma
Veri BÃ¶lme
train_test_split(): Veriyi eÄŸitim ve test setlerine bÃ¶ler. Burada verilerin %80â€™i eÄŸitim iÃ§in, %20â€™si test iÃ§in kullanÄ±lÄ±r.

Model EÄŸitimi
RandomForestClassifier(n_estimators=100, random_state=42): Random Forest sÄ±nÄ±flandÄ±rÄ±cÄ± modeli oluÅŸturulur ve eÄŸitim verisiyle eÄŸitilir.

AdÄ±m 5: Model DeÄŸerlendirmesi
Tahmin Yapma
y_pred = model.predict(X_test): Model, test verisi Ã¼zerinde tahmin yapar.

SÄ±nÄ±flandÄ±rma Raporu
classification_report(): Modelin doÄŸruluk, precision, recall gibi metriklerini raporlar.

KarÄ±ÅŸÄ±klÄ±k Matrisi
confusion_matrix(): GerÃ§ek etiketler ile tahmin edilen etiketler arasÄ±ndaki karÅŸÄ±laÅŸtÄ±rmayÄ± gÃ¶rselleÅŸtirir. Bu matriste, doÄŸru ve yanlÄ±ÅŸ sÄ±nÄ±flandÄ±rmalar yer alÄ±r.

AdÄ±m 6: Ã–zellik Ã–nem Dereceleri
model.feature_importances_: Modelin hangi Ã¶zellikleri daha Ã¶nemli bulduÄŸunu gÃ¶sterir.

importances.sort_values().plot.barh(): Ã–zelliklerin Ã¶nem derecelerini gÃ¶rselleÅŸtirir.

AdÄ±m 7: Modeli Kaydetme
joblib.dump(): Model, dosyaya kaydedilir. Bu sayede ilerleyen zamanlarda model yeniden eÄŸitilmeden kullanÄ±labilir.

Ã–zet ve SonuÃ§lar:
Veri HazÄ±rlÄ±ÄŸÄ±: XML ve CSV verisi birleÅŸtirilip, Ã¶zellik mÃ¼hendisliÄŸi yapÄ±ldÄ±.

Model EÄŸitimi: RandomForest sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ± ile stres seviyelerini tahmin etmeye yÃ¶nelik bir model oluÅŸturuldu.

DeÄŸerlendirme: Modelin performansÄ± sÄ±nÄ±flandÄ±rma raporu ve karÄ±ÅŸÄ±klÄ±k matrisi ile deÄŸerlendirildi.

Ã–zellik Ã–nemleri: Hangi Ã¶zelliklerin stres seviyesini tahmin etmede daha etkili olduÄŸu gÃ¶rselleÅŸtirildi.

Model Kaydetme: Model kaydedilerek daha sonra kullanÄ±labilir hale getirildi.

Bu adÄ±mlarla, fare iskelet hareketlerine dayalÄ± stres seviyesi tahmin eden bir makine Ã¶ÄŸrenmesi modelini oluÅŸturmuÅŸ olduk.

---


MODELÄ° GELÄ°ÅTÄ°RÄ°YORUZ
"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20]
}

grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)
grid_search.fit(X_train, y_train)

"""Bu kodumuzla, GridSearchCV kullanarak Hyperparametre Tuning (Model Parametre AyarlamasÄ±) iÅŸlemi yapmaktayÄ±z. Bu sÃ¼reÃ§, modelin performansÄ±nÄ± artÄ±rmak amacÄ±yla farklÄ± hiperparametre kombinasyonlarÄ±nÄ± test eder ve en iyi kombinasyonu seÃ§er.

AdÄ±m adÄ±m aÃ§Ä±klamaya geÃ§elim:

1. Parametre AlanÄ± (param_grid)
python
Kopyala
DÃ¼zenle
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20]
}
param_grid: Bu, modelin parametrelerinin hangi deÄŸer aralÄ±klarÄ±nda denenmesi gerektiÄŸini belirten bir sÃ¶zlÃ¼k (dictionary) tanÄ±mlar.

n_estimators: Bu parametre, RandomForestClassifier'da kaÃ§ adet aÄŸaÃ§ (decision tree) kullanÄ±lacaÄŸÄ±nÄ± belirtir. Burada Ã¼Ã§ farklÄ± deÄŸer deneniyor: 50, 100 ve 200.

max_depth: Bu parametre, her bir karar aÄŸacÄ±nÄ±n derinliÄŸini belirler. Derinlik sÄ±nÄ±rlanarak modelin aÅŸÄ±rÄ± Ã¶ÄŸrenmesini engelleyebilirsiniz. Burada Ã¼Ã§ farklÄ± deÄŸer deneniyor: None (sÄ±nÄ±rsÄ±z derinlik), 10 ve 20.

2. GridSearchCV TanÄ±mlamasÄ±
grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)
GridSearchCV: Bu, grid search algoritmasÄ±nÄ± kullanarak hiperparametre optimizasyonu yapacak olan sÄ±nÄ±ftÄ±r. AÅŸaÄŸÄ±daki parametrelerle tanÄ±mlanÄ±r:

RandomForestClassifier(): Bu, denenecek modelin RandomForestClassifier olduÄŸunu belirtir. Buradaki RandomForestClassifier() model, decision tree tabanlÄ± bir sÄ±nÄ±flandÄ±rÄ±cÄ±dÄ±r ve Ã¶zellikle karmaÅŸÄ±k verileri sÄ±nÄ±flandÄ±rmada etkili bir modeldir.

param_grid: Bu, modelin hangi parametrelerde test edileceÄŸini gÃ¶sterir. YukarÄ±da tanÄ±mladÄ±ÄŸÄ±mÄ±z parametre aralÄ±klarÄ±nÄ± iÃ§erir.

cv=5: Bu, Ã§apraz doÄŸrulama iÅŸlemi iÃ§in 5 katmanlÄ± (fold) Ã§apraz doÄŸrulama kullanacaÄŸÄ±nÄ± belirtir. Yani, veri seti 5 parÃ§aya bÃ¶lÃ¼nÃ¼r ve her seferinde 4 parÃ§a eÄŸitim, 1 parÃ§a test olarak kullanÄ±lÄ±r. Bu, modelin genellenebilirliÄŸini test etmek iÃ§in yaygÄ±n bir tekniktir.

3. Modelin EÄŸitilmesi
grid_search.fit(X_train, y_train)
fit: Bu metod, GridSearchCV'yi Ã§alÄ±ÅŸtÄ±rarak verilen parametre grid'ini kullanarak modelin eÄŸitimini yapar.

X_train: Modelin eÄŸitim verisi (Ã¶zellikler).

y_train: EÄŸitim verisine karÅŸÄ±lÄ±k gelen etiketler (stres seviyeleri).

Bu adÄ±mda GridSearchCV tÃ¼m parametre kombinasyonlarÄ±nÄ± (Ã¶rneÄŸin, 50 aÄŸaÃ§lÄ± model, 100 aÄŸaÃ§lÄ± model vs.) deneyerek her birini Ã§apraz doÄŸrulama ile test eder ve en iyi sonucu veren parametre kombinasyonunu seÃ§er.

4. GridSearchCVâ€™nin Ã‡Ä±ktÄ±larÄ±
GridSearchCV'nin fit iÅŸlemi tamamlandÄ±ktan sonra, grid_search.best_params_ ile en iyi parametreler alÄ±nabilir.
print("En iyi parametreler:", grid_search.best_params_)
Bu, en iyi sonuÃ§ veren parametre kombinasyonunu dÃ¶ndÃ¼recektir.

Ã–zetle:
GridSearchCV kullanÄ±larak, modelin hiperparametrelerinin optimize edilmesi saÄŸlanÄ±r.

Bu optimizasyon iÅŸlemi, farklÄ± parametre kombinasyonlarÄ±nÄ± deneyerek modelin performansÄ±nÄ± artÄ±rmak iÃ§in yapÄ±lÄ±r.

Ã‡apraz doÄŸrulama (cross-validation) kullanÄ±larak, modelin genel doÄŸruluÄŸu daha doÄŸru bir ÅŸekilde Ã¶lÃ§Ã¼lÃ¼r ve aÅŸÄ±rÄ± uyum (overfitting) Ã¶nlenir.
"""

# Hareketin zamansal deÄŸiÅŸimi
features['x_diff'] = features['x_mean'].diff()
features['y_diff'] = features['y_mean'].diff()

!pip install scikit-optimize

!pip install optuna

import pandas as pd
import xml.etree.ElementTree as ET

# XML verisini okuma ve iÅŸleme
def parse_xml(xml_file):
    tree = ET.parse(xml_file)
    root = tree.getroot()

    data = []

    for track in root.findall('.//track'):
        if track.attrib.get('label') != 'rat_skeleton':
            continue

        for skeleton in track.findall('skeleton'):
            frame_id = skeleton.attrib.get('frame')
            if frame_id is None:
                continue

            for point in skeleton.findall('points'):
                label = point.get('label')
                points = point.get('points')

                if points is None or label is None:
                    continue

                x, y = map(float, points.split(','))
                data.append({
                    'frame_id': int(frame_id),
                    'label': int(label),
                    'x': x,
                    'y': y
                })

    return pd.DataFrame(data)

# Veriyi yÃ¼kleyip DataFrame oluÅŸturma
df = parse_xml('annotations.xml')

# Ã–zellik mÃ¼hendisliÄŸi yaparak model iÃ§in uygun hale getirme
# Ã–rneÄŸin, x ve y'nin ortalama ve standart sapmasÄ±nÄ± hesaplayabilirsiniz
features = df.groupby('frame_id').agg({
    'x': ['mean', 'std', 'min', 'max'],
    'y': ['mean', 'std', 'min', 'max'],
}).reset_index()

# SÃ¼tun isimlerini dÃ¼zenleme
features.columns = ['frame_id', 'x_mean', 'x_std', 'x_min', 'x_max', 'y_mean', 'y_std', 'y_min', 'y_max']

# Ä°stenilen formatta veriyi almak iÃ§in bu ÅŸekilde dÃ¶nÃ¼ÅŸtÃ¼rme iÅŸlemi yapabilirsiniz.
print(features.head())

print("X boyutu:", X.shape)
print("y boyutu:", y.shape)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
import optuna



# EÄŸer y'nin her satÄ±ra birden fazla etiket atanÄ±yorsa, veri hazÄ±rlÄ±ÄŸÄ±mÄ±zÄ± buna gÃ¶re yapmalÄ±yÄ±z.

# X ve y'yi doÄŸru eÅŸleÅŸtirerek X'yi Ã¶zellikler, y'yi etiketler olarak ayÄ±rÄ±yoruz.
df_features = pd.concat([features, df[['label']]], axis=1)

# Åimdi X ve y'yi ayÄ±ralÄ±m
X = df_features.drop(columns=['frame_id', 'label'])  # Ã–zellikler
y = df_features['label']  # Etiketler

# X ve y'nin boyutlarÄ±nÄ± kontrol edelim
print("X boyutu:", X.shape)
print("y boyutu:", y.shape)

# EÄŸer X ve y'nin boyutlarÄ± uyumluysa, ÅŸimdi eÄŸitim ve test verisine ayÄ±rabiliriz.
# EÄŸitim ve test verisine ayÄ±rma
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Optuna hedef fonksiyonu
def objective(trial):
    # Optuna parametrelerini tanÄ±mla
    n_estimators = trial.suggest_int('n_estimators', 50, 200)
    max_depth = trial.suggest_int('max_depth', 10, 30)

    # RandomForestClassifier modeli oluÅŸtur
    rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth)

    # Ã‡apraz doÄŸrulama ile modelin performansÄ±nÄ± Ã¶lÃ§
    score = cross_val_score(rf, X_train, y_train, cv=5).mean()

    return score

# Optuna Ã§alÄ±ÅŸtÄ±rma
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

# En iyi parametreleri yazdÄ±r
print("En iyi parametreler:", study.best_params)

"""AÃ§Ä±klamalar:

Veri EÅŸleÅŸtirme: X ve y'yi doÄŸru bir ÅŸekilde eÅŸleÅŸtirdik ve y'nin doÄŸru ÅŸekilde her Ã¶rneÄŸe karÅŸÄ±lÄ±k gelmesini saÄŸladÄ±k.

Boyut KontrolÃ¼: X ve y'yi eÅŸleÅŸtirdikten sonra, her iki deÄŸiÅŸkenin boyutlarÄ±nÄ± kontrol ettik.

Optuna: Optuna ile hiperparametre optimizasyonu iÃ§in hedef fonksiyonu belirledik ve eÄŸitim verileri Ã¼zerinde Ã§apraz doÄŸrulama ile en iyi parametreleri bulmaya Ã§alÄ±ÅŸtÄ±k.
NOT : 1 saatten fazla Ã§alÄ±ÅŸtÄ±ÄŸÄ± iÃ§in durdurmak zorunda kaldÄ±m.
"""

!pip install hyperopt

from hyperopt import fmin, tpe, hp, Trials
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.datasets import make_classification

# Ã–rnek veri seti oluÅŸturma
X, y = make_classification(n_samples=1000, n_features=8, random_state=42)

# EÄŸitim ve test verisine ayÄ±rma
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Parametre aralÄ±klarÄ± (seÃ§ilecek deÄŸer listeleri)
n_estimators_options = [50, 100, 200, 300]
max_depth_options = [None, 10, 20, 30, 40]

# Hedef fonksiyon
def objective(params):
    rf = RandomForestClassifier(
        n_estimators=int(params['n_estimators']),
        max_depth=params['max_depth'],
        min_samples_split=int(params['min_samples_split']),
        random_state=42
    )
    score = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy').mean()
    return -score  # fmin minimize eder

# Parametre alanÄ±
space = {
    'n_estimators': hp.choice('n_estimators', n_estimators_options),
    'max_depth': hp.choice('max_depth', max_depth_options),
    'min_samples_split': hp.quniform('min_samples_split', 2, 10, 1)
}

# Hyperopt optimizasyonu
trials = Trials()
best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=100, trials=trials)

# SeÃ§ilen parametreleri listelerden geri al (Ã§Ã¼nkÃ¼ hp.choice indeks dÃ¶ndÃ¼rÃ¼r)
best['n_estimators'] = n_estimators_options[best['n_estimators']]
best['max_depth'] = max_depth_options[best['max_depth']]
best['min_samples_split'] = int(best['min_samples_split'])  # float dÃ¶nÃ¼yor

print("En iyi parametreler:", best)

# En iyi parametrelerle model eÄŸitimi
best_rf = RandomForestClassifier(
    n_estimators=best['n_estimators'],
    max_depth=best['max_depth'],
    min_samples_split=best['min_samples_split'],
    random_state=42
)
best_rf.fit(X_train, y_train)

# Test seti deÄŸerlendirmesi
print("Test seti skoru:", best_rf.score(X_test, y_test))

"""Bu kodun amacÄ±, Random Forest modelinin hiperparametrelerini otomatik ÅŸekilde optimize etmektir. KullanÄ±lan yÃ¶ntem: Hyperopt adlÄ± bir Bayesian optimizasyon kÃ¼tÃ¼phanesidir.

ğŸ” 1. KODUN GENEL AMACI NEDÄ°R?
AmaÃ§:
RandomForestClassifier modeline ait n_estimators, max_depth ve min_samples_split gibi hiperparametrelerin en iyi kombinasyonunu bulmak. Bu iÅŸlemde:

Modelin baÅŸarÄ±mÄ±nÄ± artÄ±rmak (accuracy skorunu maksimize etmek)

Parametreleri deneme-yanÄ±lma yapmadan, otomatik ve akÄ±llÄ± biÃ§imde ayarlamak

ğŸ§  2. KODUN MANTIÄI (ADIM ADIM)
ğŸ“Œ AdÄ±m 1 â€“ Veri Seti
X, y = make_classification(n_samples=1000, n_features=8, random_state=42)
Bu satÄ±r, Ã¶rnek olarak rastgele bir sÄ±nÄ±flandÄ±rma veri seti Ã¼retir. 1000 Ã¶rnek (satÄ±r) ve 8 Ã¶zellik (sÃ¼tun) vardÄ±r.

ğŸ“Œ AdÄ±m 2 â€“ EÄŸitim/Test BÃ¶lÃ¼nmesi
X_train, X_test, y_train, y_test = train_test_split(...)
Veri eÄŸitim ve test olarak %80 / %20 oranÄ±nda ikiye bÃ¶lÃ¼nÃ¼r. EÄŸitim verisi model eÄŸitimi ve Ã§apraz doÄŸrulama iÃ§in kullanÄ±lÄ±r.

ğŸ“Œ AdÄ±m 3 â€“ Hiperparametre AlanÄ±nÄ±n TanÄ±mÄ±
space = {
  'n_estimators': hp.choice('n_estimators', [50, 100, 200, 300]),
  'max_depth': hp.choice('max_depth', [None, 10, 20, 30, 40]),
  'min_samples_split': hp.quniform('min_samples_split', 2, 10, 1)
}
Bu alan (space), hangi parametrelerin hangi aralÄ±kta deneneceÄŸini belirler.


Parametre	AÃ§Ä±klama
n_estimators	Orman iÃ§inde kaÃ§ adet karar aÄŸacÄ± olacaÄŸÄ±nÄ± belirtir
max_depth	AÄŸaÃ§larÄ±n maksimum derinliÄŸi (None = sÄ±nÄ±rsÄ±z)
min_samples_split	Dallanma iÃ§in gereken minimum Ã¶rnek sayÄ±sÄ±
ğŸ“Œ AdÄ±m 4 â€“ AmaÃ§ Fonksiyonu
def objective(params): ...
Bu fonksiyon ÅŸunu yapar:

Verilen parametrelerle RandomForestClassifier modeli oluÅŸturur.

Bu modelle X_train Ã¼zerinde 5 katlÄ± Ã§apraz doÄŸrulama (cross-validation) yapÄ±lÄ±r.

Ortalama baÅŸarÄ± (accuracy) skorunu hesaplar.

Bu skoru negatif olarak dÃ¶ner. Ã‡Ã¼nkÃ¼ hyperopt.fmin() fonksiyonu minimizasyon yapar, biz ise maksimum baÅŸarÄ±yÄ± arÄ±yoruz.

ğŸ“Œ AdÄ±m 5 â€“ Optimizasyon
best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=100, trials=trials)
fmin: En iyi parametreleri bulmaya Ã§alÄ±ÅŸÄ±r.

algo=tpe.suggest: Tree-structured Parzen Estimator (TPE) algoritmasÄ± kullanÄ±lÄ±r â†’ Bu algoritma, parametre uzayÄ±nÄ± "akÄ±llÄ± ÅŸekilde" keÅŸfeder.

max_evals=100: Maksimum 100 farklÄ± parametre kombinasyonu denenir.

trials: Her denemenin sonucu bu nesneye kaydedilir.

ğŸ“Œ AdÄ±m 6 â€“ En Ä°yi Parametreler ile Model EÄŸitimi
best_rf = RandomForestClassifier(...)
best_rf.fit(X_train, y_train)
En iyi bulunan parametrelerle model yeniden eÄŸitilir ve X_test verisiyle doÄŸruluÄŸu (accuracy) hesaplanÄ±r.

ğŸ“¦ 3. KULLANILAN KÃœTÃœPHANE: Hyperopt
Neden Hyperopt?
GridSearchCV gibi klasik yÃ¶ntemler tÃ¼m kombinasyonlarÄ± dener â†’ yavaÅŸ ve maliyetli

Hyperopt, Bayesian optimizasyon yapar: Ã–nceki denemelerden Ã¶ÄŸrenerek daha mantÄ±klÄ± denemeler yapar.

Ã–zellikle Ã§ok fazla hiperparametre ve geniÅŸ aralÄ±klar olduÄŸunda daha hÄ±zlÄ± ve daha akÄ±llÄ± sonuÃ§lar verir.

ğŸ¯ SONUÃ‡TA NE ELDE EDERÄ°Z?

Kodun Ã§Ä±ktÄ±sÄ±nda:

En iyi parametreler: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 4}
Test seti skoru: 0.91
gibi bir Ã§Ä±ktÄ± alÄ±rsÄ±n.
Bunun anlamÄ± ÅŸudur:

En iyi doÄŸruluk, n_estimators=200, max_depth=20, min_samples_split=4 ile elde edildi.

Bu parametrelerle test verisinde baÅŸarÄ± %91 oldu.
"""